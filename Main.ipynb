{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Main.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aFWpuNSzGq9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edcaf9f7-3cd5-46ce-a3a2-5b4f3b99842b"
      },
      "source": [
        "import pandas\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "!pip install patool\n",
        "import patoolib\n",
        "import scipy.io\n",
        "import glob\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "drive.mount('/content/gdrive')\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: patool in /usr/local/lib/python3.7/dist-packages (1.12)\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EY6PrfV4zGq_"
      },
      "source": [
        "sim_dir =  '/content/gdrive/My Drive/פרויקט מסכם/simulations/'\n",
        "\n",
        "# for rar in glob.glob(sim_dir + '*.rar'):\n",
        "#   patoolib.extract_archive(rar, outdir=sim_dir)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset = []\n",
        "# labels = []\n",
        "# for file in glob.glob(sim_dir + '*/*.mat'):\n",
        "#   mat = scipy.io.loadmat(file)['logY'].toarray()\n",
        "#   dataset.append(mat)\n",
        "#   if \"homogenous\" in file:\n",
        "#     labels.append(0)\n",
        "#   else: \n",
        "#     labels.append(1)\n",
        "# dataset = np.array(dataset)\n",
        "# labels = np.array(labels)\n",
        "# np.save(sim_dir + \"dataset.npy\", dataset)\n",
        "# np.save(sim_dir + \"labels.npy\", labels)\n",
        "\n",
        "dataset = np.load(sim_dir + \"dataset.npy\")\n",
        "labels = np.load(sim_dir + \"labels.npy\")"
      ],
      "metadata": {
        "id": "2_7WPrhjs5Aw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.shape)\n",
        "print(labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "es9dBLWaxIpp",
        "outputId": "1d91ce4f-048a-43bc-a22a-81b1b621db89"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 1, 9)\n",
            "(10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def norm_data(data):\n",
        "  output = []\n",
        "  for sample in data: \n",
        "    sample = (sample - np.mean(sample))/np.std(sample)\n",
        "    output.append(np.array(sample))\n",
        "  return np.array(output)\n",
        "\n",
        "input_size = 9\n",
        "num_of_samples = dataset.shape[0]\n",
        "\n",
        "perm = torch.randperm(num_of_samples)\n",
        "dataset = dataset[perm]\n",
        "labels = labels[perm]\n",
        "\n",
        "x_train, x_val, x_test = dataset[:round(0.7*num_of_samples)], dataset[round(0.7*num_of_samples):round(0.9*num_of_samples)], dataset[round(0.9*num_of_samples):]\n",
        "t_train, t_val, t_test = labels[:round(0.7*num_of_samples)],  labels[round(0.7*num_of_samples):round(0.9*num_of_samples)],  labels[round(0.9*num_of_samples):]\n",
        "\n",
        "x_train_norm = norm_data(x_train)\n",
        "x_val_norm = norm_data(x_val)\n",
        "x_test_norm = norm_data(x_test)\n",
        "\n",
        "x_train_norm = np.reshape(x_train_norm, (x_train_norm.shape[0], x_train_norm.shape[1]*x_train_norm.shape[2]))\n",
        "x_val_norm = np.reshape(x_val_norm, (x_val_norm.shape[0], x_val_norm.shape[1]*x_val_norm.shape[2]))\n",
        "x_test_norm = np.reshape(x_test_norm, (x_test_norm.shape[0], x_test_norm.shape[1]*x_test_norm.shape[2]))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    np.concatenate((t_train[:,None], x_train_norm),axis=1),\n",
        "    batch_size=64, shuffle=True)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    np.concatenate((t_val[:,None], x_val_norm),axis=1),\n",
        "    batch_size=64, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    np.concatenate((t_test[:,None], x_test_norm),axis=1),\n",
        "    batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "unFA2G0nNX-i",
        "outputId": "1a106bab-787a-475f-be45-0e9d18fa8dd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7000, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, num_hidden=400, input_size=9):\n",
        "        super(PyTorchMLP, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_size, num_hidden)\n",
        "        self.layer2 = nn.Linear(num_hidden, 2)\n",
        "        self.num_hidden = num_hidden\n",
        "        self.input_size = input_size\n",
        "    def forward(self, x):\n",
        "        x = x.reshape([-1, input_size])\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x "
      ],
      "metadata": {
        "id": "BY07RPjnMdDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(model, loader=train_loader):\n",
        "    model.eval()\n",
        "    loss = 0\n",
        "    correct = 0\n",
        "    pred_list = []\n",
        "    true_list = []\n",
        "    for ar in loader:\n",
        "        data = ar[:,1:]\n",
        "        label = ar[:,0]\n",
        "        # send to device\n",
        "        data, label = data.to(device), label.to(device)\n",
        "        \n",
        "        # data = data.view(-1, 28*28)\n",
        "        # data = data.view(-1, 1, 28, 28)\n",
        "        pred = model(data)\n",
        "        loss += F.nll_loss(pred, label.long(), reduction='sum').item() # sum up batch loss                                                               \n",
        "        pred = pred.data.max(1, keepdim=True)[1] # get the index of the max log-probability                                                                 \n",
        "        correct += pred.eq(label.data.view_as(pred)).cpu().sum().item()\n",
        "   \n",
        "    loss /= len(loader.dataset)\n",
        "    accuracy = 100. * correct / len(loader.dataset)\n",
        "    \n",
        "    return loss, accuracy\n",
        "\n",
        "def train(model, lr=0.01, max_iters=1000,num_epochs=6):\n",
        "    model.train()\n",
        "    train_accs, valid_accs = [], []\n",
        "    epochs = []\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    n = 0 # the number of iterations\n",
        "    iters, losses = [], []\n",
        "    iters_sub = []\n",
        "\n",
        "    for epoch in range(0, num_epochs):\n",
        "\n",
        "        for batch_idx, ar in enumerate(train_loader):           \n",
        "            data = ar[:,1:]\n",
        "            label = ar[:,0]\n",
        "            # send to device\n",
        "            data, label = data.to(device), label.to(device)\n",
        "            # data = data.view(-1, 28*28)\n",
        "            # data = data.view(-1, 1, 28, 28)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(data)\n",
        "            loss = F.nll_loss(pred, label.long())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            iters.append(n)\n",
        "            losses.append(loss)\n",
        "\n",
        "            if batch_idx % 64 == 0: \n",
        "                \n",
        "                iters_sub.append(n)\n",
        "                train_loss, train_acc = get_accuracy(model, loader=train_loader)\n",
        "                train_accs.append(train_acc)\n",
        "\n",
        "                valid_loss, valid_acc = get_accuracy(model, loader=val_loader)\n",
        "                valid_accs.append(valid_acc)\n",
        "      \n",
        "                print(\"Iter %d. [Val Acc %.0f%%] [Train Acc %.0f%%, Loss %f]\" % (n, valid_acc, train_acc, train_loss))\n",
        "\n",
        "            # increment the iteration number\n",
        "            n += 1\n",
        "            if n > max_iters:\n",
        "                return iters, losses, iters_sub, train_accs, valid_accs\n",
        "    return iters, losses, iters_sub, train_accs, valid_accs"
      ],
      "metadata": {
        "id": "37QPep2hPf-P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
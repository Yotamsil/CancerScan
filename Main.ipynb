{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Main.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aFWpuNSzGq9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edcaf9f7-3cd5-46ce-a3a2-5b4f3b99842b"
      },
      "source": [
        "import pandas\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "!pip install patool\n",
        "import patoolib\n",
        "import scipy.io\n",
        "import glob\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "drive.mount('/content/gdrive')\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: patool in /usr/local/lib/python3.7/dist-packages (1.12)\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EY6PrfV4zGq_"
      },
      "source": [
        "sim_dir =  '/content/gdrive/My Drive/פרויקט מסכם/simulations/'\n",
        "\n",
        "# for rar in glob.glob(sim_dir + '*.rar'):\n",
        "#   patoolib.extract_archive(rar, outdir=sim_dir)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset = []\n",
        "# labels = []\n",
        "# for file in glob.glob(sim_dir + '*/*.mat'):\n",
        "#   mat = scipy.io.loadmat(file)['logY'].toarray()\n",
        "#   dataset.append(mat)\n",
        "#   if \"homogenous\" in file:\n",
        "#     labels.append(0)\n",
        "#   else: \n",
        "#     labels.append(1)\n",
        "# dataset = np.array(dataset)\n",
        "# labels = np.array(labels)\n",
        "# np.save(sim_dir + \"dataset.npy\", dataset)\n",
        "# np.save(sim_dir + \"labels.npy\", labels)\n",
        "\n",
        "dataset = np.load(sim_dir + \"dataset.npy\")\n",
        "labels = np.load(sim_dir + \"labels.npy\")"
      ],
      "metadata": {
        "id": "2_7WPrhjs5Aw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.shape)\n",
        "print(labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "es9dBLWaxIpp",
        "outputId": "1d91ce4f-048a-43bc-a22a-81b1b621db89"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 1, 9)\n",
            "(10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def norm_data(data):\n",
        "  output = []\n",
        "  for sample in data: \n",
        "    sample = (sample - np.mean(sample))/np.std(sample)\n",
        "    output.append(np.array(sample))\n",
        "  return np.array(output)\n",
        "\n",
        "input_size = 9\n",
        "num_of_samples = dataset.shape[0]\n",
        "\n",
        "perm = torch.randperm(num_of_samples)\n",
        "dataset = dataset[perm]\n",
        "labels = labels[perm]\n",
        "\n",
        "x_train, x_val, x_test = dataset[:round(0.7*num_of_samples)], dataset[round(0.7*num_of_samples):round(0.9*num_of_samples)], dataset[round(0.9*num_of_samples):]\n",
        "t_train, t_val, t_test = labels[:round(0.7*num_of_samples)],  labels[round(0.7*num_of_samples):round(0.9*num_of_samples)],  labels[round(0.9*num_of_samples):]\n",
        "\n",
        "x_train_norm = norm_data(x_train)\n",
        "x_val_norm = norm_data(x_val)\n",
        "x_test_norm = norm_data(x_test)\n",
        "\n",
        "x_train_norm = np.reshape(x_train_norm, (x_train_norm.shape[0], x_train_norm.shape[1]*x_train_norm.shape[2]))\n",
        "x_val_norm = np.reshape(x_val_norm, (x_val_norm.shape[0], x_val_norm.shape[1]*x_val_norm.shape[2]))\n",
        "x_test_norm = np.reshape(x_test_norm, (x_test_norm.shape[0], x_test_norm.shape[1]*x_test_norm.shape[2]))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    np.concatenate((t_train[:,None], x_train_norm),axis=1),\n",
        "    batch_size=64, shuffle=True)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    np.concatenate((t_val[:,None], x_val_norm),axis=1),\n",
        "    batch_size=64, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    np.concatenate((t_test[:,None], x_test_norm),axis=1),\n",
        "    batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unFA2G0nNX-i",
        "outputId": "1a106bab-787a-475f-be45-0e9d18fa8dd0"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7000, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_size=9, num_hidden=50):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_size, num_hidden)\n",
        "        self.layer2 = nn.Linear(num_hidden, num_hidden//2)\n",
        "        self.layer3 = nn.Linear(num_hidden//2, 2)\n",
        "        self.num_hidden = num_hidden\n",
        "        self.input_size = input_size\n",
        "    def forward(self, x):\n",
        "        x = x.reshape([-1, input_size])\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x "
      ],
      "metadata": {
        "id": "BY07RPjnMdDU"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(model, loader=train_loader):\n",
        "    model.eval()\n",
        "    loss = 0\n",
        "    correct = 0\n",
        "    pred_list = []\n",
        "    true_list = []\n",
        "    for ar in loader:\n",
        "        data = ar[:,1:]\n",
        "        label = ar[:,0]\n",
        "        # send to device\n",
        "        data, label = data.to(device), label.to(device)\n",
        "        \n",
        "        data = data.view(-1, 9)\n",
        "        data = data.view(-1, 1, 9, 1)\n",
        "        pred = model(data)\n",
        "        loss += F.nll_loss(pred, label.long(), reduction='sum').item() # sum up batch loss                                                               \n",
        "        pred = pred.data.max(1, keepdim=True)[1] # get the index of the max log-probability                                                                 \n",
        "        correct += pred.eq(label.data.view_as(pred)).cpu().sum().item()\n",
        "    # print(f\"correct = {correct}\")\n",
        "    # print(f\"num_of_ars = {num_of_ars}\")\n",
        "    loss /= len(loader.dataset)\n",
        "    accuracy = 100. * correct / len(loader.dataset)\n",
        "    \n",
        "    return loss, accuracy\n",
        "\n",
        "def train(model, lr=0.01, max_iters=1000,num_epochs=6):\n",
        "    model.train()\n",
        "    train_accs, valid_accs = [], []\n",
        "    epochs = []\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    n = 0 # the number of iterations\n",
        "    iters, losses = [], []\n",
        "    iters_sub = []\n",
        "\n",
        "    for epoch in range(0, num_epochs):\n",
        "\n",
        "        for batch_idx, ar in enumerate(train_loader):           \n",
        "            data = ar[:,1:]\n",
        "            label = ar[:,0]\n",
        "            # send to device\n",
        "            data, label = data.to(device), label.to(device)\n",
        "            # data = data.view(-1, 28*28)\n",
        "            # data = data.view(-1, 1, 28, 28)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(data)\n",
        "            loss = F.nll_loss(pred, label.long())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            iters.append(n)\n",
        "            losses.append(loss)\n",
        "\n",
        "            if batch_idx % 64 == 0: \n",
        "                \n",
        "                iters_sub.append(n)\n",
        "                train_loss, train_acc = get_accuracy(model, loader=train_loader)\n",
        "                train_accs.append(train_acc)\n",
        "\n",
        "                valid_loss, valid_acc = get_accuracy(model, loader=val_loader)\n",
        "                valid_accs.append(valid_acc)\n",
        "      \n",
        "                print(\"Iter %d. [Val Acc %.0f%%] [Train Acc %.0f%%, Loss %f]\" % (n, valid_acc, train_acc, train_loss))\n",
        "\n",
        "            # increment the iteration number\n",
        "            n += 1\n",
        "            if n > max_iters:\n",
        "                return iters, losses, iters_sub, train_accs, valid_accs\n",
        "    return iters, losses, iters_sub, train_accs, valid_accs"
      ],
      "metadata": {
        "id": "37QPep2hPf-P"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = NeuralNetwork(input_size, 20)\n",
        "model.to(device)\n",
        "\n",
        "lr = 0.0001\n",
        "\n",
        "iters, losses, iters_sub, train_accs, valid_accs = train(model.double(), lr=lr, max_iters=10000, num_epochs=15)"
      ],
      "metadata": {
        "id": "D3meXB8WZo9T",
        "outputId": "d5240a99-c543-40bf-d82f-d05b538f0154",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 0. [Val Acc 20%] [Train Acc 20%, Loss -0.411191]\n",
            "Iter 64. [Val Acc 20%] [Train Acc 20%, Loss -0.478383]\n",
            "Iter 110. [Val Acc 80%] [Train Acc 80%, Loss -0.532014]\n",
            "Iter 174. [Val Acc 80%] [Train Acc 80%, Loss -0.604607]\n",
            "Iter 220. [Val Acc 80%] [Train Acc 80%, Loss -0.652987]\n",
            "Iter 284. [Val Acc 80%] [Train Acc 80%, Loss -0.703607]\n",
            "Iter 330. [Val Acc 80%] [Train Acc 80%, Loss -0.728165]\n",
            "Iter 394. [Val Acc 80%] [Train Acc 80%, Loss -0.751346]\n",
            "Iter 440. [Val Acc 80%] [Train Acc 80%, Loss -0.762710]\n",
            "Iter 504. [Val Acc 80%] [Train Acc 80%, Loss -0.773093]\n",
            "Iter 550. [Val Acc 80%] [Train Acc 80%, Loss -0.778350]\n",
            "Iter 614. [Val Acc 80%] [Train Acc 80%, Loss -0.783341]\n",
            "Iter 660. [Val Acc 80%] [Train Acc 80%, Loss -0.786021]\n",
            "Iter 724. [Val Acc 80%] [Train Acc 80%, Loss -0.788705]\n",
            "Iter 770. [Val Acc 80%] [Train Acc 80%, Loss -0.790226]\n",
            "Iter 834. [Val Acc 80%] [Train Acc 80%, Loss -0.791888]\n",
            "Iter 880. [Val Acc 80%] [Train Acc 80%, Loss -0.792772]\n",
            "Iter 944. [Val Acc 80%] [Train Acc 80%, Loss -0.793777]\n",
            "Iter 990. [Val Acc 80%] [Train Acc 80%, Loss -0.794373]\n",
            "Iter 1054. [Val Acc 80%] [Train Acc 80%, Loss -0.795057]\n",
            "Iter 1100. [Val Acc 80%] [Train Acc 80%, Loss -0.795465]\n",
            "Iter 1164. [Val Acc 80%] [Train Acc 80%, Loss -0.795944]\n",
            "Iter 1210. [Val Acc 80%] [Train Acc 80%, Loss -0.796236]\n",
            "Iter 1274. [Val Acc 80%] [Train Acc 80%, Loss -0.796583]\n",
            "Iter 1320. [Val Acc 80%] [Train Acc 80%, Loss -0.796796]\n",
            "Iter 1384. [Val Acc 80%] [Train Acc 80%, Loss -0.797055]\n",
            "Iter 1430. [Val Acc 80%] [Train Acc 80%, Loss -0.797216]\n",
            "Iter 1494. [Val Acc 80%] [Train Acc 80%, Loss -0.797404]\n",
            "Iter 1540. [Val Acc 80%] [Train Acc 80%, Loss -0.797535]\n",
            "Iter 1604. [Val Acc 80%] [Train Acc 80%, Loss -0.797694]\n"
          ]
        }
      ]
    }
  ]
}